{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n        \nfrom sklearn import metrics\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-14T17:29:40.979402Z","iopub.execute_input":"2022-12-14T17:29:40.979792Z","iopub.status.idle":"2022-12-14T17:29:50.950660Z","shell.execute_reply.started":"2022-12-14T17:29:40.979760Z","shell.execute_reply":"2022-12-14T17:29:50.949722Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/faces-age-detection-dataset/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-12-14T17:10:29.155538Z","iopub.execute_input":"2022-12-14T17:10:29.156222Z","iopub.status.idle":"2022-12-14T17:10:29.179799Z","shell.execute_reply.started":"2022-12-14T17:10:29.156183Z","shell.execute_reply":"2022-12-14T17:10:29.178925Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"dataset.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T17:10:29.181304Z","iopub.execute_input":"2022-12-14T17:10:29.181945Z","iopub.status.idle":"2022-12-14T17:10:29.200603Z","shell.execute_reply.started":"2022-12-14T17:10:29.181911Z","shell.execute_reply":"2022-12-14T17:10:29.199489Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"          ID   Class\n0    377.jpg  MIDDLE\n1  17814.jpg   YOUNG\n2  21283.jpg  MIDDLE\n3  16496.jpg   YOUNG\n4   4487.jpg  MIDDLE\n5   6283.jpg  MIDDLE\n6  23495.jpg   YOUNG\n7   7100.jpg   YOUNG\n8   6028.jpg   YOUNG\n9  22617.jpg     OLD","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>377.jpg</td>\n      <td>MIDDLE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>17814.jpg</td>\n      <td>YOUNG</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>21283.jpg</td>\n      <td>MIDDLE</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>16496.jpg</td>\n      <td>YOUNG</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4487.jpg</td>\n      <td>MIDDLE</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6283.jpg</td>\n      <td>MIDDLE</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>23495.jpg</td>\n      <td>YOUNG</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7100.jpg</td>\n      <td>YOUNG</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>6028.jpg</td>\n      <td>YOUNG</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>22617.jpg</td>\n      <td>OLD</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dataset['Class'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-12-14T17:10:29.202803Z","iopub.execute_input":"2022-12-14T17:10:29.203427Z","iopub.status.idle":"2022-12-14T17:10:29.215773Z","shell.execute_reply.started":"2022-12-14T17:10:29.203390Z","shell.execute_reply":"2022-12-14T17:10:29.214732Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"array(['MIDDLE', 'YOUNG', 'OLD'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"dataset['Class'].hist()","metadata":{"execution":{"iopub.status.busy":"2022-12-14T17:10:29.217538Z","iopub.execute_input":"2022-12-14T17:10:29.218228Z","iopub.status.idle":"2022-12-14T17:10:29.468396Z","shell.execute_reply.started":"2022-12-14T17:10:29.218195Z","shell.execute_reply":"2022-12-14T17:10:29.467471Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"dataset['Class'].replace(['YOUNG', 'MIDDLE','OLD'], [0, 1, 2], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T17:10:29.471362Z","iopub.execute_input":"2022-12-14T17:10:29.472172Z","iopub.status.idle":"2022-12-14T17:10:29.486224Z","shell.execute_reply.started":"2022-12-14T17:10:29.472144Z","shell.execute_reply":"2022-12-14T17:10:29.485413Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom keras import backend as K\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D\nfrom keras.callbacks import TensorBoard\nfrom sklearn.metrics import classification_report,confusion_matrix\nimport itertools","metadata":{"execution":{"iopub.status.busy":"2022-12-14T17:10:29.488616Z","iopub.execute_input":"2022-12-14T17:10:29.489221Z","iopub.status.idle":"2022-12-14T17:10:36.013800Z","shell.execute_reply.started":"2022-12-14T17:10:29.489186Z","shell.execute_reply":"2022-12-14T17:10:36.012806Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Methods for loading images.\ndef imageReader(path,ch = 3, resize=(64,64)):\n    kp = tf.io.read_file(path)\n    kp = tf.image.decode_jpeg(kp, channels=ch)\n    kp = tf.image.convert_image_dtype(kp, dtype=tf.float32)\n    kp = tf.image.resize(kp, resize)\n    return kp\n\ndef load_data(image_path, label):\n    image = imageReader(image_path, 3, (64,64))\n    \n    return (image, label)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T17:10:37.547930Z","iopub.execute_input":"2022-12-14T17:10:37.548560Z","iopub.status.idle":"2022-12-14T17:10:37.556949Z","shell.execute_reply.started":"2022-12-14T17:10:37.548525Z","shell.execute_reply":"2022-12-14T17:10:37.555178Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import glob\n# Initializing image paths variables.\nimage_paths = glob.glob('/kaggle/input/faces-age-detection-dataset/Train/*.jpg')\nprint(len(image_paths))","metadata":{"execution":{"iopub.status.busy":"2022-12-14T17:10:40.469444Z","iopub.execute_input":"2022-12-14T17:10:40.469831Z","iopub.status.idle":"2022-12-14T17:10:40.528875Z","shell.execute_reply.started":"2022-12-14T17:10:40.469799Z","shell.execute_reply":"2022-12-14T17:10:40.527869Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"19906\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize label values\nlabel_list = []\nfor i in image_paths:\n    _,tail = os.path.split(i)\n    label = dataset.loc[dataset['ID'] == tail]['Class'].values[0]\n    label_list.append(label)\n# print(len(label_list))","metadata":{"execution":{"iopub.status.busy":"2022-12-14T17:10:43.112712Z","iopub.execute_input":"2022-12-14T17:10:43.113722Z","iopub.status.idle":"2022-12-14T17:11:17.251429Z","shell.execute_reply.started":"2022-12-14T17:10:43.113661Z","shell.execute_reply":"2022-12-14T17:11:17.250211Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\n# Create Train, Test, and Validation datasets\ntrain_size = int(0.8*(len(image_paths)))\ntest_size = 20\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((image_paths[:train_size], label_list[:train_size]))\ntest_dataset = tf.data.Dataset.from_tensor_slices((image_paths[train_size:train_size + test_size], label_list[train_size :train_size + test_size]))\nvalidation_dataset = tf.data.Dataset.from_tensor_slices((image_paths[train_size + test_size:], label_list[train_size + test_size:]))","metadata":{"execution":{"iopub.status.busy":"2022-12-14T17:11:28.494165Z","iopub.execute_input":"2022-12-14T17:11:28.494527Z","iopub.status.idle":"2022-12-14T17:11:28.614441Z","shell.execute_reply.started":"2022-12-14T17:11:28.494497Z","shell.execute_reply":"2022-12-14T17:11:28.613524Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Creating Autotune objects\n# For Autotune objects the prefetch buffer sizes are automitically tuned. \nAUTOTUNE = tf.data.AUTOTUNE\ntrain_dataset = (train_dataset\n    .map(load_data, num_parallel_calls=AUTOTUNE)\n    .batch(64)\n    .prefetch(AUTOTUNE)\n)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-14T17:11:37.518335Z","iopub.execute_input":"2022-12-14T17:11:37.518728Z","iopub.status.idle":"2022-12-14T17:11:37.642997Z","shell.execute_reply.started":"2022-12-14T17:11:37.518691Z","shell.execute_reply":"2022-12-14T17:11:37.642041Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Adding data agumentation to training images.\n# Data agumentation adds variation to training data\ntrain_dataset = train_dataset.map(\n    lambda image, label: (tf.image.random_flip_left_right(image), label)\n).cache(\n).map(\n    lambda image, label: (tf.image.per_image_standardization(image), label)\n).map(\n    lambda image, label: (tf.image.random_contrast(image, lower=0.4, upper=0.6), label)\n).map(\n    lambda image, label: (tf.image.random_brightness(image, max_delta = 0.4), label)\n).map(\n    lambda image, label: (tf.image.random_hue(image, max_delta = 0.4), label)\n).map(\n    lambda image, label: (tf.image.random_saturation(image, lower=0.4, upper=0.6), label)\n).shuffle(\n    1000\n).repeat(2)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T17:11:39.253388Z","iopub.execute_input":"2022-12-14T17:11:39.253778Z","iopub.status.idle":"2022-12-14T17:11:39.428589Z","shell.execute_reply.started":"2022-12-14T17:11:39.253746Z","shell.execute_reply":"2022-12-14T17:11:39.427637Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE\nvalidation_dataset = (validation_dataset\n    .map(load_data, num_parallel_calls=AUTOTUNE)\n    .batch(64)\n    .prefetch(AUTOTUNE)\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T17:11:40.503384Z","iopub.execute_input":"2022-12-14T17:11:40.503793Z","iopub.status.idle":"2022-12-14T17:11:40.524460Z","shell.execute_reply.started":"2022-12-14T17:11:40.503757Z","shell.execute_reply":"2022-12-14T17:11:40.523389Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Adding data agumentation to validation images.\nvalidation_dataset = validation_dataset.map(\n    lambda image, label: (tf.image.random_flip_left_right(image), label)\n).cache(\n).map(\n    lambda image, label: (tf.image.per_image_standardization(image), label)\n).map(\n    lambda image, label: (tf.image.random_contrast(image, lower=0.4, upper=0.6), label)\n).map(\n    lambda image, label: (tf.image.random_brightness(image, max_delta = 0.4), label)\n).map(\n    lambda image, label: (tf.image.random_hue(image, max_delta = 0.4), label)\n).map(\n    lambda image, label: (tf.image.random_saturation(image, lower=0.4, upper=0.6), label)\n).shuffle(\n    1000\n).repeat(2)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T17:11:41.972567Z","iopub.execute_input":"2022-12-14T17:11:41.973588Z","iopub.status.idle":"2022-12-14T17:11:42.132662Z","shell.execute_reply.started":"2022-12-14T17:11:41.973549Z","shell.execute_reply":"2022-12-14T17:11:42.131638Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"test_dataset = (test_dataset\n    .map(load_data, num_parallel_calls=AUTOTUNE)\n    .batch(64)\n    .prefetch(AUTOTUNE)\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T17:11:42.923116Z","iopub.execute_input":"2022-12-14T17:11:42.924062Z","iopub.status.idle":"2022-12-14T17:11:42.943495Z","shell.execute_reply.started":"2022-12-14T17:11:42.924017Z","shell.execute_reply":"2022-12-14T17:11:42.942643Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Standardizing the test data.\ntest_dataset = test_dataset.map(\n    lambda image, label: (tf.image.per_image_standardization(image), label)\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T17:11:44.520495Z","iopub.execute_input":"2022-12-14T17:11:44.521620Z","iopub.status.idle":"2022-12-14T17:11:44.558191Z","shell.execute_reply.started":"2022-12-14T17:11:44.521570Z","shell.execute_reply":"2022-12-14T17:11:44.557287Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Using VGG 16\npreprocess_input = tf.keras.applications.vgg16.preprocess_input\nmodel = tf.keras.applications.VGG16(input_shape = (64,64,3,),include_top=False,weights='imagenet')\n","metadata":{"execution":{"iopub.status.busy":"2022-12-14T17:11:54.818716Z","iopub.execute_input":"2022-12-14T17:11:54.819081Z","iopub.status.idle":"2022-12-14T17:11:55.755577Z","shell.execute_reply.started":"2022-12-14T17:11:54.819051Z","shell.execute_reply":"2022-12-14T17:11:55.754480Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n58892288/58889256 [==============================] - 0s 0us/step\n58900480/58889256 [==============================] - 0s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set model.trainable to false, this prevents changing weights of pre-trained model.\nmodel.trainable = False\n\n# Adding custom layers to the pre-trained model.\ninputs = tf.keras.Input(shape=(64, 64, 3))\nx = preprocess_input(inputs)\nx = model(x, training=True)\n# x = tf.keras.layers.GlobalAveragePooling2D()(x)\nx = tf.keras.layers.Flatten()(x)\nx = tf.keras.layers.Dense(1000)(x)\nx = tf.keras.layers.Dense(600)(x)\nx = tf.keras.layers.Dense(600)(x)\nx = tf.keras.layers.Dropout(0.2)(x)\nx = tf.keras.layers.Dense(400)(x)\nx = tf.keras.layers.Dense(400)(x)\nx = tf.keras.layers.Dense(100)(x)\nx = tf.keras.layers.Dense(100)(x)\nx = tf.keras.layers.Dropout(0.2)(x)\nx = tf.keras.layers.Dense(50)(x)\nx = tf.keras.layers.Dense(50)(x)\nx = tf.keras.layers.Dense(20)(x)\nx = tf.keras.layers.Dense(20)(x)\noutputs = tf.keras.layers.Dense(3)(x)\nmodel = tf.keras.Model(inputs, outputs)\n\n# Compiling the model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(),\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))","metadata":{"execution":{"iopub.status.busy":"2022-12-14T17:17:50.589051Z","iopub.execute_input":"2022-12-14T17:17:50.589419Z","iopub.status.idle":"2022-12-14T17:17:51.131134Z","shell.execute_reply.started":"2022-12-14T17:17:50.589389Z","shell.execute_reply":"2022-12-14T17:17:51.130205Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-12-14T17:17:51.168174Z","iopub.execute_input":"2022-12-14T17:17:51.168470Z","iopub.status.idle":"2022-12-14T17:17:51.179463Z","shell.execute_reply.started":"2022-12-14T17:17:51.168444Z","shell.execute_reply":"2022-12-14T17:17:51.178454Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Model: \"model_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_8 (InputLayer)         [(None, 64, 64, 3)]       0         \n_________________________________________________________________\ntf.__operators__.getitem_6 ( (None, 64, 64, 3)         0         \n_________________________________________________________________\ntf.nn.bias_add_6 (TFOpLambda (None, 64, 64, 3)         0         \n_________________________________________________________________\nmodel_4 (Functional)         (None, 3)                 23352133  \n_________________________________________________________________\nflatten_5 (Flatten)          (None, 3)                 0         \n_________________________________________________________________\ndense_55 (Dense)             (None, 1000)              4000      \n_________________________________________________________________\ndense_56 (Dense)             (None, 600)               600600    \n_________________________________________________________________\ndense_57 (Dense)             (None, 600)               360600    \n_________________________________________________________________\ndropout_10 (Dropout)         (None, 600)               0         \n_________________________________________________________________\ndense_58 (Dense)             (None, 400)               240400    \n_________________________________________________________________\ndense_59 (Dense)             (None, 400)               160400    \n_________________________________________________________________\ndense_60 (Dense)             (None, 100)               40100     \n_________________________________________________________________\ndense_61 (Dense)             (None, 100)               10100     \n_________________________________________________________________\ndropout_11 (Dropout)         (None, 100)               0         \n_________________________________________________________________\ndense_62 (Dense)             (None, 50)                5050      \n_________________________________________________________________\ndense_63 (Dense)             (None, 50)                2550      \n_________________________________________________________________\ndense_64 (Dense)             (None, 20)                1020      \n_________________________________________________________________\ndense_65 (Dense)             (None, 20)                420       \n_________________________________________________________________\ndense_66 (Dense)             (None, 3)                 63        \n=================================================================\nTotal params: 24,777,436\nTrainable params: 1,425,303\nNon-trainable params: 23,352,133\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"history = model.fit(train_dataset, epochs=20, validation_data=validation_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T17:18:00.822830Z","iopub.execute_input":"2022-12-14T17:18:00.823199Z","iopub.status.idle":"2022-12-14T17:28:08.919201Z","shell.execute_reply.started":"2022-12-14T17:18:00.823168Z","shell.execute_reply":"2022-12-14T17:28:08.915106Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Epoch 1/20\n498/498 [==============================] - 22s 35ms/step - loss: 0.9555 - sparse_categorical_crossentropy: 0.9555 - val_loss: 0.9699 - val_sparse_categorical_crossentropy: 0.9698\nEpoch 2/20\n498/498 [==============================] - 21s 34ms/step - loss: 0.9532 - sparse_categorical_crossentropy: 0.9532 - val_loss: 0.9587 - val_sparse_categorical_crossentropy: 0.9586\nEpoch 3/20\n498/498 [==============================] - 22s 34ms/step - loss: 0.9528 - sparse_categorical_crossentropy: 0.9528 - val_loss: 0.9634 - val_sparse_categorical_crossentropy: 0.9633\nEpoch 4/20\n498/498 [==============================] - 22s 34ms/step - loss: 0.9526 - sparse_categorical_crossentropy: 0.9525 - val_loss: 0.9590 - val_sparse_categorical_crossentropy: 0.9589\nEpoch 5/20\n498/498 [==============================] - 22s 34ms/step - loss: 0.9524 - sparse_categorical_crossentropy: 0.9524 - val_loss: 0.9588 - val_sparse_categorical_crossentropy: 0.9587\nEpoch 6/20\n498/498 [==============================] - 22s 34ms/step - loss: 1.1001 - sparse_categorical_crossentropy: 1.1000 - val_loss: 0.9588 - val_sparse_categorical_crossentropy: 0.9587\nEpoch 7/20\n498/498 [==============================] - 22s 34ms/step - loss: 0.9520 - sparse_categorical_crossentropy: 0.9520 - val_loss: 0.9596 - val_sparse_categorical_crossentropy: 0.9595\nEpoch 8/20\n498/498 [==============================] - 22s 34ms/step - loss: 0.9522 - sparse_categorical_crossentropy: 0.9522 - val_loss: 0.9587 - val_sparse_categorical_crossentropy: 0.9587\nEpoch 9/20\n498/498 [==============================] - 21s 34ms/step - loss: 0.9520 - sparse_categorical_crossentropy: 0.9520 - val_loss: 0.9600 - val_sparse_categorical_crossentropy: 0.9599\nEpoch 10/20\n498/498 [==============================] - 21s 34ms/step - loss: 0.9521 - sparse_categorical_crossentropy: 0.9521 - val_loss: 0.9589 - val_sparse_categorical_crossentropy: 0.9588\nEpoch 11/20\n498/498 [==============================] - 21s 35ms/step - loss: 0.9521 - sparse_categorical_crossentropy: 0.9521 - val_loss: 0.9591 - val_sparse_categorical_crossentropy: 0.9590\nEpoch 12/20\n498/498 [==============================] - 22s 34ms/step - loss: 0.9520 - sparse_categorical_crossentropy: 0.9520 - val_loss: 0.9586 - val_sparse_categorical_crossentropy: 0.9585\nEpoch 13/20\n498/498 [==============================] - 22s 36ms/step - loss: 0.9519 - sparse_categorical_crossentropy: 0.9519 - val_loss: 0.9595 - val_sparse_categorical_crossentropy: 0.9594\nEpoch 14/20\n498/498 [==============================] - 22s 34ms/step - loss: 0.9521 - sparse_categorical_crossentropy: 0.9520 - val_loss: 0.9586 - val_sparse_categorical_crossentropy: 0.9585\nEpoch 15/20\n498/498 [==============================] - 22s 34ms/step - loss: 0.9521 - sparse_categorical_crossentropy: 0.9521 - val_loss: 0.9595 - val_sparse_categorical_crossentropy: 0.9594\nEpoch 16/20\n498/498 [==============================] - 22s 35ms/step - loss: 0.9520 - sparse_categorical_crossentropy: 0.9520 - val_loss: 0.9588 - val_sparse_categorical_crossentropy: 0.9587\nEpoch 17/20\n498/498 [==============================] - 21s 34ms/step - loss: 0.9523 - sparse_categorical_crossentropy: 0.9523 - val_loss: 0.9598 - val_sparse_categorical_crossentropy: 0.9597\nEpoch 18/20\n498/498 [==============================] - 21s 34ms/step - loss: 0.9522 - sparse_categorical_crossentropy: 0.9522 - val_loss: 0.9590 - val_sparse_categorical_crossentropy: 0.9589\nEpoch 19/20\n498/498 [==============================] - 21s 34ms/step - loss: 0.9520 - sparse_categorical_crossentropy: 0.9519 - val_loss: 0.9584 - val_sparse_categorical_crossentropy: 0.9583\nEpoch 20/20\n498/498 [==============================] - 22s 34ms/step - loss: 0.9520 - sparse_categorical_crossentropy: 0.9520 - val_loss: 0.9586 - val_sparse_categorical_crossentropy: 0.9585\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"accuracy:   %0.3f\" % metrics.accuracy_score(actual_values, predictions))","metadata":{"execution":{"iopub.status.busy":"2022-12-14T17:30:23.158772Z","iopub.execute_input":"2022-12-14T17:30:23.159185Z","iopub.status.idle":"2022-12-14T17:30:23.168602Z","shell.execute_reply.started":"2022-12-14T17:30:23.159151Z","shell.execute_reply":"2022-12-14T17:30:23.167530Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"accuracy:   0.400\n","output_type":"stream"}]}]}